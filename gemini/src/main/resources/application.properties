# Application Name
spring.application.name=gemini

# Server Configuration
server.port=8080

# Ollama AI Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3.2

# Connection Timeout Settings
spring.ai.ollama.connection.timeout=5000
  # Connection timeout in milliseconds
spring.ai.ollama.connection.read-timeout=10000
  # Read timeout in milliseconds

# Thread Pool Configuration for Async Processing
spring.task.execution.pool.size=10
  # Core pool size
spring.task.execution.pool.max-size=20
  # Maximum pool size
spring.task.execution.pool.queue-capacity=50
  # Queue capacity

# Caching Configuration
spring.cache.type=simple  # Use simple caching
spring.cache.cache-names=ollamaResponses  # Name of the cache
spring.cache.cache-ttl=300  # Time-to-live for cache entries in seconds

# Ollama Model Options
spring.ai.ollama.chat.options.keep_alive=5m
  # Keep the model loaded for 5 minutes
spring.ai.ollama.chat.options.temperature=0.7
  # Adjust response randomness